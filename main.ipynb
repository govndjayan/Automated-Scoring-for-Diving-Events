{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "9cyLha8w6pvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install htbuilder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDZqtdXu6nsj",
        "outputId": "8e0dcb9e-43fd-4215-c2c9-85e9a6294ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting htbuilder\n",
            "  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from htbuilder) (9.0.0)\n",
            "Building wheels for collected packages: htbuilder\n",
            "  Building wheel for htbuilder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htbuilder: filename=htbuilder-0.6.1-py3-none-any.whl size=12456 sha256=4d6ce9e70445b7cf0da5600266e8e62db77f1225b1622356a53779495c30041a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/21/e6/de289dcc715fbd7b770e3b4cf81103ffc2f3cdc0d8090889bf\n",
            "Successfully built htbuilder\n",
            "Installing collected packages: htbuilder\n",
            "Successfully installed htbuilder-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFSLuT1c6lDU",
        "outputId": "f7dc5679-3624-471d-e2a0-845711255106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.20.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.0.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.19.6)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (12.6.0)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.13.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.29)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.9)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.1->streamlit) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2022.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (1.24.3)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit) (2.6.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPOJWFlDSWQv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from models.C3D_altered import C3D_altered\n",
        "from models.my_fc6 import my_fc6\n",
        "from models.score_regressor import score_regressor\n",
        "from models.C3D_model import C3D\n",
        "import streamlit_analytics\n",
        "from opts import *\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import os\n",
        "import cv2 as cv\n",
        "import tempfile\n",
        "from torchvision import transforms\n",
        "\n",
        "import urllib\n",
        "from htbuilder import HtmlElement, div, ul, li, br, hr, a, p, img, styles, classes, fonts\n",
        "from htbuilder.units import percent, px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(randomseed)\n",
        "torch.cuda.manual_seed_all(randomseed)\n",
        "random.seed(randomseed)\n",
        "np.random.seed(randomseed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "WCIyrxj9Uwug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_path = os.path.abspath(os.getcwd())\n",
        "m1_path = os.path.join(current_path, m1_path)\n",
        "m2_path = os.path.join(current_path, m2_path)\n",
        "m3_path = os.path.join(current_path, m3_path)\n",
        "c3d_path = os.path.join(current_path, c3d_path)"
      ],
      "metadata": {
        "id": "hCzp2dQ9Uz-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "CVqNCnW0U3JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def im_convert(tensor):  \n",
        "  image=tensor.cpu().clone().detach().numpy()    \n",
        "  image=image.squeeze()  \n",
        "  image=image.transpose(1,2,0)  \n",
        "  image=image*np.array((0.485, 0.456, 0.406))+np.array((0.229, 0.224, 0.225))  \n",
        "  return image.clip(0,1)\n",
        "    "
      ],
      "metadata": {
        "id": "IvpYA7PDpSUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def center_crop(img, dim):\n",
        "    \"\"\"Returns center cropped image\n",
        "    Args:Image Scaling\n",
        "    img: image to be center cropped\n",
        "    dim: dimensions (width, height) to be cropped from center\n",
        "    \"\"\"\n",
        "    width, height = img.shape[1], img.shape[0]\n",
        "    #process crop width and height for max available dimension\n",
        "    crop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
        "    crop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0]\n",
        "    mid_x, mid_y = int(width/2), int(height/2)\n",
        "    cw2, ch2 = int(crop_width/2), int(crop_height/2)\n",
        "    crop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
        "    return crop_img"
      ],
      "metadata": {
        "id": "ZxbCVsFyU6Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def action_classifier(frames):\n",
        "    # C3D raw\n",
        "    model_C3D = C3D()\n",
        "    model_C3D.load_state_dict(torch.load(c3d_path, map_location={'cuda:0': 'cpu'}))\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        X = torch.zeros((1, 3, 16, 112, 112))\n",
        "        frames2keep = np.linspace(0, frames.shape[2] - 1, 16, dtype=int)\n",
        "        ctr = 0\n",
        "        for i in frames2keep:\n",
        "            X[:, :, ctr, :, :] = frames[:, :, i, :, :]\n",
        "            ctr += 1\n",
        "        print(\"print alter:\", X)\n",
        "        \n",
        "\n",
        "        # modifying\n",
        "        model_C3D.eval()\n",
        "\n",
        "        # perform prediction\n",
        "        X = X*255\n",
        "        X = torch.flip(X, [1])\n",
        "        prediction = model_C3D(X)\n",
        "        prediction = prediction.data.cpu().numpy()\n",
        "\n",
        "        # print top predictions\n",
        "        top_inds = prediction[0].argsort()[::-1][:5]  # reverse sort and take five largest items\n",
        "        print('\\nTop 5:')\n",
        "        print('Top inds: ', top_inds)\n",
        "    return top_inds[0]"
      ],
      "metadata": {
        "id": "hvlOrgR3U9QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_one_video(video_file):\n",
        "   if video_file != \"sample\":\n",
        "        tfile = tempfile.NamedTemporaryFile(delete=False)\n",
        "        tfile.write(video_file.read())\n",
        "\n",
        "        vf = cv.VideoCapture(tfile.name)\n",
        "   else:\n",
        "        vf = cv.VideoCapture(\"054.avi\")\n",
        "   frames = None\n",
        "   while vf.isOpened():\n",
        "        ret, frame = vf.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "        frame = cv.resize(frame, input_resize, interpolation=cv.INTER_LINEAR) #frame resized: (128, 171, 3)\n",
        "        frame = center_crop(frame, (H, H))       \n",
        "        frame = transform(frame).unsqueeze(0)\n",
        "       # frame = im_convert(frame)\n",
        "        if frames is not None:\n",
        "            frames = np.vstack((frames, frame))          \n",
        "        else:\n",
        "            frames = frame           \n",
        "   vf.release()\n",
        "   print('frames shape: ', frames.shape)\n",
        "   cv.destroyAllWindows()\n",
        "   rem = len(frames) % 16\n",
        "   rem = 16 - rem\n",
        "   print('frames shape: ', frames.shape)\n",
        "   if rem != 0:\n",
        "        padding = np.zeros((rem, C, H, H))\n",
        "        print(\"padding shape:\", padding.shape)\n",
        "        frames = np.vstack((frames, padding))\n",
        "# frames shape: (137, 3, 112, 112)\n",
        "   frames = torch.from_numpy(frames).unsqueeze(0)\n",
        "   print(f\"video shape: {frames.shape}\") # video shape: torch.Size([1, 144, 3, 112, 112])\n",
        "   frames = frames.transpose_(1, 2)\n",
        "   frames = frames.double()\n",
        "   return frames\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WDwWAzDMVDHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8ERmDrCgpca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_my_fc6 = my_fc6()\n",
        "#model_scripted = torch.jit.script(model_CNN) # Export to TorchScript\n",
        "#model_scripted.save('model_my_fc6_94.pt') # Save"
      ],
      "metadata": {
        "id": "lhlFw1xljrWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN= C3D_altered()\n",
        "torch.save(model_CNN.state_dict(),\"model_CNN_94.pth\")"
      ],
      "metadata": {
        "id": "UF7yqQOh9ywD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model_CNN = C3D_altered()\n",
        "pickle.dump(model_CNN, open('model_CNN_94.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "IW_k9BUeIpJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_my_fc6 = my_fc6()\n",
        "torch.save(model_my_fc6.state_dict(),\"model_my_fc6_94.pth\")"
      ],
      "metadata": {
        "id": "TaDr0RCM-UOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_with_one_video_frames(frames):\n",
        "    action_class = action_classifier(frames)\n",
        "    if action_class != 463:\n",
        "        return None\n",
        "  #loading our C3D layer\n",
        "    model_CNN= C3D_altered()\n",
        "    model_CNN.load_state_dict(torch.load(m1_path, map_location={'cuda:0': 'cpu'}))\n",
        "    model_CNN.eval()\n",
        "  # loading our fc6 layer\n",
        "    model_my_fc6 = my_fc6()\n",
        "    model_my_fc6.load_state_dict(torch.load(m2_path,map_location=None))\n",
        "\n",
        "  # loading our score regressor\n",
        "    model_score_regressor = score_regressor()\n",
        "    model_score_regressor.load_state_dict(torch.load(m3_path, map_location=None))\n",
        "    with torch.no_grad():\n",
        "        pred_scores = []\n",
        "\n",
        "        model_CNN.eval()\n",
        "        model_my_fc6.eval()\n",
        "        model_score_regressor.eval()\n",
        "\n",
        "        clip_feats = torch.Tensor([])\n",
        "        print(f\"frames shape: {frames.shape}\")\n",
        "        for i in np.arange(0, frames.shape[2], 16):\n",
        "            clip = frames[:, :, i:i + 16, :, :]\n",
        "            model_CNN = model_CNN.double()\n",
        "            clip_feats_temp = model_CNN(clip)\n",
        "\n",
        "            # clip_feats_temp shape: torch.Size([1, 8192])\n",
        "\n",
        "            clip_feats_temp.unsqueeze_(0)\n",
        "\n",
        "            # clip_feats_temp unsqueeze shape: torch.Size([1, 1, 8192])\n",
        "\n",
        "            clip_feats_temp.transpose_(0, 1)\n",
        "\n",
        "            # clip_feats_temp transposes shape: torch.Size([1, 1, 8192])\n",
        "\n",
        "            clip_feats = torch.cat((clip_feats.double(), clip_feats_temp), 1)\n",
        "\n",
        "            # clip_feats shape: torch.Size([1, 1, 8192])\n",
        "\n",
        "        clip_feats_avg = clip_feats.mean(1)\n",
        "\n",
        "\n",
        "        model_my_fc6 = model_my_fc6.double()\n",
        "        sample_feats_fc6 = model_my_fc6(clip_feats_avg)\n",
        "        model_score_regressor = model_score_regressor.double()\n",
        "        temp_final_score = model_score_regressor(sample_feats_fc6)\n",
        "        print(temp_final_score)\n",
        "        pred_scores.extend([element[0] for element in temp_final_score.data.cpu().numpy()])\n",
        "        print(pred_scores)\n",
        "        print(pred_scores[0])\n",
        "        \n",
        "        return pred_scores"
      ],
      "metadata": {
        "id": "aJAiJluHVIdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image(src_as_string, **style):\n",
        "    return img(src=src_as_string, style=styles(**style))\n",
        "\n",
        "\n",
        "def link(link, text, **style):\n",
        "    return a(_href=link, _target=\"_blank\", style=styles(**style))(text)\n",
        "\n",
        "\n",
        "def layout(*args):\n",
        "\n",
        "    style = \"\"\"\n",
        "    <style>\n",
        "      footer {visibility: hidden;}\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    style_div = styles(\n",
        "        position=\"fixed\",\n",
        "        left=0,\n",
        "        bottom=0,\n",
        "        margin=px(0, 0, 0, 0),\n",
        "        width=percent(100),\n",
        "        color=\"pink\",\n",
        "        text_align=\"center\",\n",
        "        height=\"auto\",\n",
        "        opacity=1\n",
        "    )\n",
        "\n",
        "    body = p()\n",
        "    foot = div(\n",
        "        style=style_div\n",
        "    )(\n",
        "        body\n",
        "    )\n",
        "\n",
        "    st.markdown(style, unsafe_allow_html=True)\n",
        "\n",
        "    for arg in args:\n",
        "        if isinstance(arg, str):\n",
        "            body(arg)\n",
        "\n",
        "        elif isinstance(arg, HtmlElement):\n",
        "            body(arg)\n",
        "\n",
        "    st.markdown(str(foot), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "def footer():\n",
        "    myargs = [\n",
        "        \"By Angela Aswin Dylan Govind\",\n",
        "      \n",
        "        br(),\n",
        "    ]\n",
        "    layout(*myargs)"
      ],
      "metadata": {
        "id": "EUmZ1pQtVMVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(video_file):\n",
        "    if video_file is not None or video_file == \"sample\":\n",
        "        # Display a message while perdicting\n",
        "        val = 0\n",
        "        res_img = st.empty()\n",
        "        res_msg = st.empty()\n",
        "\n",
        "        # Making prediction\n",
        "        frames = preprocess_one_video(video_file)\n",
        "        if frames.shape[2] > 400:\n",
        "            res_msg.error(\"The uploaded video is too long.\")\n",
        "        else:\n",
        "            preds = inference_with_one_video_frames(frames)\n",
        "            if preds is None:\n",
        "                res_img.empty()\n",
        "                res_msg.error(\"The uploaded video does not seem to be a diving video.\")\n",
        "            else:\n",
        "                val = int(preds[0] * 17)\n",
        "\n",
        "                # Clear waiting messages and show results\n",
        "                print(f\"Predicted score after multiplication: {val}\")\n",
        "                res_img.empty()\n",
        "                res_msg.success(\"Predicted score: {}\".format(val))\n"
      ],
      "metadata": {
        "id": "nV9GP2a8Vani"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    with st.spinner('Loading to welcome you...'):\n",
        "        \n",
        "\n",
        "        st.title(\"Automated Scoring For Diving Events ---- Powered by AI\")\n",
        "        st.subheader(\"Upload Olympics diving video and check its AI predicted score\")\n",
        "        footer()\n",
        "\n",
        "        video_file = st.file_uploader(\"Upload a video here\", type=[\"mp4\", \"mov\", \"avi\"])\n",
        "\n",
        "        if video_file is None:\n",
        "            st.subheader(\"Don't have Olympics diving videos? Try the sample video below.\")\n",
        "            diving_img = st.empty()\n",
        "            if st.button(\"Sample Video\"):\n",
        "                diving_img.empty()\n",
        "                diving_img.image(\n",
        "                    \"https://raw.githubusercontent.com/gitskim/AQA_Streamlit/main/054.gif\",\n",
        "                    width = 300)\n",
        "                col2 = st.empty()\n",
        "                col2.markdown(\"Actual Score: 84.15\")\n",
        "                col2_msg = st.empty()\n",
        "                col2_msg.error(\"Please wait. Making predictions now...\")\n",
        "                video_file=\"sample\"\n",
        "                make_prediction(video_file)\n",
        "                col2_msg.empty()\n",
        "\n",
        "        else:\n",
        "            # Display a message while perdicting\n",
        "            val = 0\n",
        "            res_img = st.empty()\n",
        "            res_msg = st.empty()\n",
        "            col1, col2, col3 = st.columns([1,1,1])\n",
        "            with col2:\n",
        "                res_img.image(\n",
        "                    \"https://media.tenor.com/images/eab0c68ee47331c4b86d679633e6d7bc/tenor.gif\",\n",
        "                    width = 100)\n",
        "                res_msg.markdown(\"### _Making Prediction now..._\")\n",
        "\n",
        "            # Making prediction\n",
        "            frames = preprocess_one_video(video_file)\n",
        "            if frames.shape[2] > 400:\n",
        "                res_msg.error(\"The uploaded video is too long.\")\n",
        "            else:\n",
        "                preds = inference_with_one_video_frames(frames)\n",
        "                if preds is None:\n",
        "                    res_img.empty()\n",
        "                    res_msg.error(\"The uploaded video does not seem to be a diving video.\")\n",
        "                else:\n",
        "                    val = int(preds[0] * 17)\n",
        "                   \n",
        "\n",
        "                    # Clear waiting messages and show results\n",
        "                    print(f\"Predicted score after multiplication: {val}\")\n",
        "                    res_img.empty()\n",
        "                    res_msg.success(\"Predicted score: {}\".format(val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od08r8GVVeHO",
        "outputId": "ee112369-0445-4ca3-f8b8-b99e9132d1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:\n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2022-11-17 04:38:49.280 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    }
  ]
}